---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# %matplotlib inline
```

# Load dataset

```{python}
boston_dataset = load_boston()
```

```{python}
boston_dataset.data.shape
```

```{python}
boston_dataset.target.shape
```

```{python}
boston_dataset.feature_names
```

```{python}
type(boston_dataset.DESCR)
```

```{python}
print(boston_dataset.DESCR)
```

```{python}
df = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
```

```{python}
df.head()
```

Add Target column 'MEDV' to dataframe

```{python}
df['MEDV'] = boston_dataset.target
```

```{python}
df.head()
```

```{python}
df.describe()
```

```{python}
df.isnull().sum() # No missing values
```

# Visualize the dataset

```{python}
plt.plot('MEDV', data=df)
plt.show()
```

```{python}
sns.pairplot(df)
```

```{python}
# Check Correlation using Heatmap
sns.set(rc={'figure.figsize':(16, 10)})
corr_mat = df.corr().round(2)
sns.heatmap(corr_mat, annot=True)
```

From above correlation matrix, we can observe:
1. Target 'MEDV' is +ve-correlated to RM(0.7) and -ve-correlated to LSTAT(-0.74)
2. Features 'NOX' & 'INDUS' are +ve correlated (0.76)
3. Features 'AGE' is +ve correlated with 'INDUS'(0.64), 'NOX' (0.73)
4. Features 'DIS' is +ve correlated with 'ZN'(0.66) and -ve correlated with 'INDUS' (-0.71), 'NOX' (-0.77), 'AGE' (-0.75)
5. Features 'RAD' is +ve correlated with 'CRIM'(0.63), 'INDUS' (.6), 'NOX' (0.61)
6. Features 'TAX' is +ve correlated with 'INDUS'(0.72), 'NOX' (.67), 'RAD' (0.91)
7. Features 'LSTAT' is +ve correlated with 'INDUS'(0.6), 'AGE' (0.6) and -ve correlated with 'RM' (-0.61)

Hence, we choose following features for our model training
features_selected = ['RM', 'LSTAT', 'CRIM', 'ZN', 'CHAS', 'NOX', 'PTRATIO', 'B']


# Define function to calculate RMSE, R-Squared for features selected

```{python}
def plot_features_vs_target(features):
    plt.figure(figsize=(20,5))

    y = df['MEDV']
    
    for i, col in enumerate(features):
        plt.subplot(1, len(features), i+1)
        x = df[col]
        plt.scatter(x, y, marker='o')
        plt.title(col)
        plt.xlabel(col)
        plt.ylabel('MEDV')
        
def display_metrics(features):
    plot_features_vs_target(features)
    
    X = df[features]
    Y = df['MEDV']
    
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)
    model = LinearRegression()
    model.fit(X_train, Y_train)
    Y_predict = model.predict(X_test)
    
    print('Root Mean squared error=', np.sqrt(mean_squared_error(Y_test, Y_predict)))
    print('R-Squared=',r2_score(Y_test, Y_predict))
```

# Model 1

```{python}
features_selected = ['RM', 'LSTAT']

display_metrics(features_selected)
```

# Model 2

```{python}
features_selected = ['RM', 'LSTAT', 'NOX']

display_metrics(features_selected)
```

# Model 3

```{python}
features_selected = ['RM', 'LSTAT', 'B']

display_metrics(features_selected)
```

# Model 4

```{python}
features_selected = ['RM', 'LSTAT', 'B','PTRATIO']

display_metrics(features_selected)
```

# Model 5

```{python}
features_selected = ['RM', 'LSTAT', 'B','PTRATIO','CHAS']

display_metrics(features_selected)
```

# Model 6

```{python}
features_selected = ['RM', 'LSTAT', 'B','PTRATIO','CHAS', 'CRIM']

display_metrics(features_selected)
```

# Model 7

```{python}
features_selected = ['RM', 'LSTAT', 'B','PTRATIO','CHAS', 'NOX']

display_metrics(features_selected)
```

# Model 8

```{python}
features_selected = ['RM', 'LSTAT', 'B','PTRATIO','CHAS', 'ZN']

display_metrics(features_selected)
```

# Model 9

```{python}
# Take all features
features_selected  = df.columns.drop('MEDV')

display_metrics(features_selected)
```

# Model 10

```{python}
# Take all features - TAX (As RAD and TAX are highly correlated)
features_selected  = df.columns.drop(['TAX', 'MEDV'])

display_metrics(features_selected)
```

# Final model (Model 9)

```{python}
# Take all features
features_selected  = df.columns.drop('MEDV')

display_metrics(features_selected)
```

R-Squared = 0.76 is good for the model
